# 简介  
kaggle入门题目，训练数据已经处理成向量并与标签一一对应，判断测试数据对应的标签。
1.	算法概述
kNN算法又称为k近邻分类(k-nearest neighbor classification)算法。
最简单平凡的分类器也许是那种死记硬背式的分类器，记住所有的训练数据，对于新的数据则直接和训练数据匹配，如果存在相同属性的训练数据，则直接用它的分类来作为新数据的分类。这种方式有一个明显的缺点，那就是很可能无法找到完全匹配的训练记录。
kNN算法则是从训练集中找到和新数据最接近的k条记录，然后根据他们的主要分类来决定新数据的类别。该算法涉及3个主要因素：训练集、距离或相似的衡量、k的大小。
2.	算法要点
2.1	计算步骤如下：
1）	算距离：计算测试数据与各个训练数据之间的距离；
2）	做排序：按照距离的递增关系进行排序；
3）	找邻居：选取距离最小的K个点；
4）	算频率：确定前K个点所在类别的出现频率；
5）	做分类：返回前K个点中出现频率最高的类别作为测试数据的预测分类。
2.2	类别的判定
1）	投票决定：少数服从多数，近邻中哪个类别的点最多就分为该类。
2）	加权投票法：根据距离的远近，对近邻的投票进行加权，距离越近则权重越大（权重为距离平方的倒数）

